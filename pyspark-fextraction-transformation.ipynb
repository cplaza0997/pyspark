{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4033f9f1-7daf-4160-85b0-e26508974055",
   "metadata": {},
   "source": [
    "# Setup dependencies\n",
    "I will be using pandas and sklearn for managing data and machine learning.\n",
    "<details>\n",
    "    <summary>pip install...</summary>\n",
    "\n",
    "```python\n",
    "# Allows to install a python package\n",
    "pip install package-name\n",
    "# or install python package with a specific version\n",
    "pip install package-name==version\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f763aec1-ed85-4e63-865f-6fe32e6878c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T03:10:19.105648Z",
     "iopub.status.busy": "2024-12-27T03:10:19.104643Z",
     "iopub.status.idle": "2024-12-27T03:10:21.347292Z",
     "shell.execute_reply": "2024-12-27T03:10:21.345303Z",
     "shell.execute_reply.started": "2024-12-27T03:10:19.105648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install PySpark version 3.1.2 silently\n",
    "#!pip install pyspark==3.1.2 -q\n",
    "# Install findSpark silently\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9432fb5f-cb46-4fe8-8ae6-8dbd060b1c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T03:10:21.348270Z",
     "iopub.status.busy": "2024-12-27T03:10:21.348270Z",
     "iopub.status.idle": "2024-12-27T03:10:21.361568Z",
     "shell.execute_reply": "2024-12-27T03:10:21.360559Z",
     "shell.execute_reply.started": "2024-12-27T03:10:21.348270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Used to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2eaa03-e6f3-4828-a197-0b8d95fb63fd",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1daee36-429d-41b7-9f25-fff7aadabdfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T21:48:52.307726Z",
     "iopub.status.busy": "2024-12-27T21:48:52.303335Z",
     "iopub.status.idle": "2024-12-27T21:48:58.155362Z",
     "shell.execute_reply": "2024-12-27T21:48:58.154361Z",
     "shell.execute_reply.started": "2024-12-27T21:48:52.307726Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# Initializing FindSpark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cae2d-a81a-4a7e-9fb8-09b052c2cf58",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98edf6f4-3350-4098-8c0d-66a70eeff6c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T21:48:58.157372Z",
     "iopub.status.busy": "2024-12-27T21:48:58.157372Z",
     "iopub.status.idle": "2024-12-27T21:49:09.987330Z",
     "shell.execute_reply": "2024-12-27T21:49:09.986169Z",
     "shell.execute_reply.started": "2024-12-27T21:48:58.157372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |sentence                                     |\n",
      "+---+---------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |\n",
      "|2  |It provides interfaces for multiple languages|\n",
      "|3  |Spark is built on top of Hadoop              |\n",
      "+---+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    [(1, \"Spark is a distributed computing system.\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")], \n",
    "    \n",
    "    [\"id\", \"sentence\"]\n",
    ")\n",
    "sentenceDataFrame.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e917e9-7b97-4214-a280-b9930cf97ff2",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "It is used to break a sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b670f71b-ded4-4482-ad40-b42d042817a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T21:49:09.988377Z",
     "iopub.status.busy": "2024-12-27T21:49:09.988377Z",
     "iopub.status.idle": "2024-12-27T21:49:17.141262Z",
     "shell.execute_reply": "2024-12-27T21:49:17.140262Z",
     "shell.execute_reply.started": "2024-12-27T21:49:09.988377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                     |words                                               |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n",
      "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
      "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Instantiate\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# Tokenize\n",
    "token_df = tokenizer.transform(sentenceDataFrame)\n",
    "# Display the tokenized data\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ecdf9-60dc-481a-a4fc-77a64cbae1e4",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "It is used to convert text into numerical format. It gives the count of each word in a given document.\n",
    "\n",
    "It returns:\n",
    "- Total number of words\n",
    "- List of index of words (unordered) of the row\n",
    "- Frequency of each word corresponding to these indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476add08-2269-4e96-a582-c9311dd17ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T21:49:17.145261Z",
     "iopub.status.busy": "2024-12-27T21:49:17.145261Z",
     "iopub.status.idle": "2024-12-27T21:49:23.737083Z",
     "shell.execute_reply": "2024-12-27T21:49:23.735661Z",
     "shell.execute_reply.started": "2024-12-27T21:49:17.145261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|id |words                                            |\n",
      "+---+-------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
      "+---+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a sample dataframe and display it.\n",
    "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63d2f13-889e-4bd6-bd13-00f2db228bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:03:28.823142Z",
     "iopub.status.busy": "2024-12-27T22:03:28.822142Z",
     "iopub.status.idle": "2024-12-27T22:03:40.773805Z",
     "shell.execute_reply": "2024-12-27T22:03:40.772431Z",
     "shell.execute_reply.started": "2024-12-27T22:03:28.823142Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Spark', 1: 'Python', 2: 'I', 3: 'love', 4: 'big', 5: 'the', 6: 'provides', 7: 'of', 8: 'API', 9: 'solves', 10: 'data', 11: 'problem', 12: 'supports'}\n",
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "|id |words                                            |features                                            |\n",
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,3,6,8],[2.0,1.0,1.0,1.0,1.0,1.0])        |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,3,12],[1.0,2.0,1.0,1.0,1.0])             |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,4,5,7,9,10,11],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "# Specify the column to be count vectorized as inputcol\n",
    "# and the output column name where the count vectors are to be stored.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "# Fit the CountVectorizer model on the input data\n",
    "model = cv.fit(textdata)\n",
    "\n",
    "# Transform the input data to bag-of-words vectors\n",
    "result = model.transform(textdata)\n",
    "\n",
    "# Print index and related word\n",
    "print({ index: word for index, word in enumerate(model.vocabulary)})\n",
    "\n",
    "# display the dataframe\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ae4aa-a887-45b4-a27e-f389b514d773",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ee75fe-e96c-44a0-88c4-09c0d44dd568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:11:03.697389Z",
     "iopub.status.busy": "2024-12-27T22:11:03.697389Z",
     "iopub.status.idle": "2024-12-27T22:11:10.303463Z",
     "shell.execute_reply": "2024-12-27T22:11:10.301909Z",
     "shell.execute_reply.started": "2024-12-27T22:11:03.697389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "|id |sentence             |\n",
      "+---+---------------------+\n",
      "|1  |Spark supports python|\n",
      "|2  |Spark is fast        |\n",
      "|3  |Spark is easy        |\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a sample dataframe and display it.\n",
    "sentenceData = spark.createDataFrame([\n",
    "        (1, \"Spark supports python\"),\n",
    "        (2, \"Spark is fast\"),\n",
    "        (3, \"Spark is easy\")\n",
    "    ], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba84c30-fcb3-489e-a203-e76b6206b025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:11:10.307057Z",
     "iopub.status.busy": "2024-12-27T22:11:10.306014Z",
     "iopub.status.idle": "2024-12-27T22:11:16.837306Z",
     "shell.execute_reply": "2024-12-27T22:11:16.835285Z",
     "shell.execute_reply.started": "2024-12-27T22:11:10.307057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+\n",
      "|id |sentence             |words                    |\n",
      "+---+---------------------+-------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |\n",
      "|3  |Spark is easy        |[spark, is, easy]        |\n",
      "+---+---------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "#tokenize the \"sentence\" column and store in the column \"words\"\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b14513-df17-4046-9340-feec8c07a9f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:11:16.838300Z",
     "iopub.status.busy": "2024-12-27T22:11:16.838300Z",
     "iopub.status.idle": "2024-12-27T22:11:23.320739Z",
     "shell.execute_reply": "2024-12-27T22:11:23.319515Z",
     "shell.execute_reply.started": "2024-12-27T22:11:16.838300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|id |sentence             |words                    |rawFeatures               |\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|(10,[4,6,9],[1.0,1.0,1.0])|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |(10,[3,6,9],[1.0,1.0,1.0])|\n",
      "|3  |Spark is easy        |[spark, is, easy]        |(10,[0,6,9],[1.0,1.0,1.0])|\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a HashingTF object\n",
    "# mention the \"words\" column as input\n",
    "# mention the \"rawFeatures\" column as output\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513a46f2-0de5-4668-ade2-20a8c91620c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:11:23.322689Z",
     "iopub.status.busy": "2024-12-27T22:11:23.322689Z",
     "iopub.status.idle": "2024-12-27T22:11:29.584034Z",
     "shell.execute_reply": "2024-12-27T22:11:29.583034Z",
     "shell.execute_reply.started": "2024-12-27T22:11:23.322689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an IDF object\n",
    "# mention the \"rawFeatures\" column as input\n",
    "# mention the \"features\" column as output\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f889ffba-dacd-4527-93bf-1098cee7e68e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T22:11:29.584592Z",
     "iopub.status.busy": "2024-12-27T22:11:29.584592Z",
     "iopub.status.idle": "2024-12-27T22:11:35.421872Z",
     "shell.execute_reply": "2024-12-27T22:11:35.420897Z",
     "shell.execute_reply.started": "2024-12-27T22:11:29.584592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------------------+\n",
      "|sentence             |features                                 |\n",
      "+---------------------+-----------------------------------------+\n",
      "|Spark supports python|(10,[4,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is fast        |(10,[3,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is easy        |(10,[0,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "+---------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the tf-idf data\n",
    "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbabb8-4f85-4df2-a7db-6ede9e41df82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:34:06.241408Z",
     "iopub.status.busy": "2024-12-27T23:34:06.240399Z",
     "iopub.status.idle": "2024-12-27T23:34:06.255058Z",
     "shell.execute_reply": "2024-12-27T23:34:06.254042Z",
     "shell.execute_reply.started": "2024-12-27T23:34:06.241408Z"
    }
   },
   "source": [
    "# StopWordsRemover\n",
    "StopWordsRemover is a transformer that filters out stop words like \"a\",\"an\" and \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "954a2404-f6a1-491a-b978-d54a502cf618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:34:31.303581Z",
     "iopub.status.busy": "2024-12-27T23:34:31.303581Z",
     "iopub.status.idle": "2024-12-27T23:34:37.577450Z",
     "shell.execute_reply": "2024-12-27T23:34:37.575929Z",
     "shell.execute_reply.started": "2024-12-27T23:34:31.303581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+\n",
      "|id |sentence                                                    |\n",
      "+---+------------------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
      "+---+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "#create a dataframe with sample text and display it\n",
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "800ad7cf-4057-44dc-bfa4-6ef15802590d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:34:56.562499Z",
     "iopub.status.busy": "2024-12-27T23:34:56.562499Z",
     "iopub.status.idle": "2024-12-27T23:35:03.004309Z",
     "shell.execute_reply": "2024-12-27T23:35:03.003028Z",
     "shell.execute_reply.started": "2024-12-27T23:34:56.562499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                                    |filtered_sentence                                   |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
    "textData = remover.transform(textData)\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e266b88-dba4-4421-9fbb-55bdc154d821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:35:32.706046Z",
     "iopub.status.busy": "2024-12-27T23:35:32.704800Z",
     "iopub.status.idle": "2024-12-27T23:35:32.723846Z",
     "shell.execute_reply": "2024-12-27T23:35:32.722842Z",
     "shell.execute_reply.started": "2024-12-27T23:35:32.706046Z"
    }
   },
   "source": [
    "# StringIndexer\n",
    "StringIndexer converts a column of strings into a column of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7340078-b563-4530-8b0e-9d16568af0c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:36:12.540653Z",
     "iopub.status.busy": "2024-12-27T23:36:12.537132Z",
     "iopub.status.idle": "2024-12-27T23:36:18.752404Z",
     "shell.execute_reply": "2024-12-27T23:36:18.751401Z",
     "shell.execute_reply.started": "2024-12-27T23:36:12.540653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  0|   red|\n",
      "|  1|   red|\n",
      "|  2|  blue|\n",
      "|  3|yellow|\n",
      "|  4|yellow|\n",
      "|  5|yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])\n",
    "\n",
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0eaa33-95d7-434f-954c-6eb85bf1ebf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:36:40.398196Z",
     "iopub.status.busy": "2024-12-27T23:36:40.397189Z",
     "iopub.status.idle": "2024-12-27T23:36:53.698618Z",
     "shell.execute_reply": "2024-12-27T23:36:53.697612Z",
     "shell.execute_reply.started": "2024-12-27T23:36:40.398196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| color|colorIndex|\n",
      "+---+------+----------+\n",
      "|  0|   red|       1.0|\n",
      "|  1|   red|       1.0|\n",
      "|  2|  blue|       2.0|\n",
      "|  3|yellow|       0.0|\n",
      "|  4|yellow|       0.0|\n",
      "|  5|yellow|       0.0|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "indexed = indexer.fit(colors).transform(colors)\n",
    "\n",
    "# display the dataframe\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d78a09-7d44-45a6-abf4-e12e2ff7e6e8",
   "metadata": {},
   "source": [
    "# StandardScaler\n",
    "StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c04e4b5e-a3f8-4fb7-84a7-1aeabf9ba7d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:37:56.887675Z",
     "iopub.status.busy": "2024-12-27T23:37:56.886747Z",
     "iopub.status.idle": "2024-12-27T23:38:03.400467Z",
     "shell.execute_reply": "2024-12-27T23:38:03.398743Z",
     "shell.execute_reply.started": "2024-12-27T23:37:56.887675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1| [70.0,170.0,17.0]|\n",
      "|  2| [80.0,165.0,25.0]|\n",
      "|  3|[65.0,150.0,135.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Create a sample dataframe and display it\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa03630-febc-4b30-9a2e-3671e3d2c16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T23:38:53.043603Z",
     "iopub.status.busy": "2024-12-27T23:38:53.043603Z",
     "iopub.status.idle": "2024-12-27T23:39:05.598522Z",
     "shell.execute_reply": "2024-12-27T23:39:05.597526Z",
     "shell.execute_reply.started": "2024-12-27T23:38:53.043603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------------------------+\n",
      "|id |features          |scaledFeatures                                             |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254367,-0.6369487984517485]|\n",
      "|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.3202563076101752,-0.5156252177942725]|\n",
      "|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021] |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the StandardScaler transformer\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "\n",
    "# Fit the transformer to the dataset\n",
    "scalerModel = scaler.fit(df)\n",
    "\n",
    "# Scale the data\n",
    "scaledData = scalerModel.transform(df)\n",
    "\n",
    "# Show the scaled data\n",
    "scaledData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b66f7-335d-4290-9315-a020fa2aa00c",
   "metadata": {},
   "source": [
    "# Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb679fa-1cbb-4665-9ad4-9e84c1f1195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T06:39:59.553421Z",
     "iopub.status.busy": "2024-12-27T06:39:59.553421Z",
     "iopub.status.idle": "2024-12-27T06:40:00.074416Z",
     "shell.execute_reply": "2024-12-27T06:40:00.073013Z",
     "shell.execute_reply.started": "2024-12-27T06:39:59.553421Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
